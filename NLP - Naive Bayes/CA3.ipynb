{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Intelligence\n",
    "## Computer Assignment 3 - Naive bayes\n",
    "### Amirali Raygan - 810197623\n",
    "###### Using 2-days Grace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project we are going to design an assistant to recognize positive comments from the negative ones. In orther to do this we use Digikala sample comments to calculate basic probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## At First \n",
    "we try to make a simple naive bayes system and step by step upgrade the effeciency.\n",
    "This codes are the simplest ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "from hazm import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"comment_train.csv\")\n",
    "p_recommendation = len(train_data[train_data['recommend'] == 'recommended']) / len(train_data['recommend'])\n",
    "t_recommendation = len(train_data[train_data['recommend'] == 'recommended'])\n",
    "n_recommendation = len(train_data[train_data['recommend'] == 'not_recommended'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "Before modeling the naive bayes we have to preprocess on the data. In this part we seperate all the words used in the train file and calculate the times each of them reapet in negative/positive comments. Then we calculate the probability of each word depend on wheter the comment is positive or negative.   \n",
    "P(word | positive) and P(word | positive). This process was done for each word using in positive commends and negative ones. Seprately for title words and comment words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  Preprocessing Comments\n",
    "#  Recommended Comments\n",
    "positive_comment_words = []\n",
    "for i in train_data[ train_data['recommend'] == 'recommended' ]['comment']:\n",
    "    a = i.split(' ')\n",
    "    for j in a:\n",
    "        positive_comment_words.append(j)\n",
    "\n",
    "total = len(positive_comment_words)\n",
    "comment_words_prob = {}\n",
    "for i in positive_comment_words:\n",
    "    if i in comment_words_prob:\n",
    "        comment_words_prob[i] += 1\n",
    "    else:\n",
    "        comment_words_prob[i] = 1\n",
    "\n",
    "for i in comment_words_prob:\n",
    "    comment_words_prob[i] = comment_words_prob[i] / (t_recommendation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not Recommended Comements\n",
    "negative_comment_words = []\n",
    "for i in train_data[ train_data['recommend'] == 'not_recommended' ]['comment']:\n",
    "    a = i.split(' ')\n",
    "    for j in a:\n",
    "        negative_comment_words.append(j)\n",
    "\n",
    "total = len(negative_comment_words)\n",
    "ncomment_words_prob = {}\n",
    "for i in negative_comment_words:\n",
    "    if i in ncomment_words_prob:\n",
    "        ncomment_words_prob[i] += 1\n",
    "    else:\n",
    "        ncomment_words_prob[i] = 1\n",
    "\n",
    "for i in ncomment_words_prob:\n",
    "    ncomment_words_prob[i] = (ncomment_words_prob[i]) / (n_recommendation) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Recommended Titles\n",
    "positive_title_words = []\n",
    "for i in train_data[ train_data['recommend'] == 'recommended' ]['title']:\n",
    "    a = i.split(' ')\n",
    "    for j in a:\n",
    "        positive_title_words.append(j)\n",
    "\n",
    "total = len(positive_title_words)\n",
    "title_words_prob = {}\n",
    "for i in positive_title_words:\n",
    "    if i in title_words_prob:\n",
    "        title_words_prob[i] += 1\n",
    "    else:\n",
    "        title_words_prob[i] = 1\n",
    "\n",
    "for i in title_words_prob:\n",
    "    title_words_prob[i] = title_words_prob[i] / (t_recommendation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Not Recommended Titles\n",
    "negative_title_words = []\n",
    "for i in train_data[ train_data['recommend'] == 'recommended' ]['title']:\n",
    "    a = i.split(' ')\n",
    "    for j in a:\n",
    "        negative_title_words.append(j)\n",
    "\n",
    "total = len(negative_title_words)\n",
    "ntitle_words_prob = {}\n",
    "for i in negative_title_words:\n",
    "    if i in ntitle_words_prob:\n",
    "        ntitle_words_prob[i] += 1\n",
    "    else:\n",
    "        ntitle_words_prob[i] = 1\n",
    "\n",
    "for i in ntitle_words_prob:\n",
    "    ntitle_words_prob[i] = ntitle_words_prob[i] / (n_recommendation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction phase\n",
    "In this part we are trying to calculate the probability of each comment to be positive or negative depend on seeing each word.\n",
    "According to bayes rule we have:                                 \n",
    "P( positive , word) = P(Word | positive) * P(Positive) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evidence \n",
    "The probability of each word in positive comment and in negative comment is our evidence. Also the probability of a comment to be positive or not is another evidence. Both of them can be calculated with train file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Liklihood\n",
    "This contains the probablity of each comment to be positive or not.(recommended or not recommended) we calculate it with previous dictionary which store the probability of each word in train file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior\n",
    "All of the probabilities which can be calculated by train file are our priors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posterior\n",
    "This contains the probabilities that are questioned. Specificly we want to claculate the probability of each comment to be positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prediction phase\n",
    "test_data = pd.read_csv(\"comment_test.csv\")\n",
    "correct = 0\n",
    "recom_correct = 0\n",
    "recom_pred = 0\n",
    "for j in range( 1, len( test_data ) ):\n",
    "    comment_words = test_data['comment'][j].split(' ')\n",
    "    recommend_prob = 0\n",
    "    not_recommend_prob = 0\n",
    "    for i in comment_words:\n",
    "        if i in comment_words_prob:\n",
    "            recommend_prob += math.log(comment_words_prob[i])\n",
    "        else:\n",
    "            recommend_prob -= 10000\n",
    "        if i in ncomment_words_prob:\n",
    "            not_recommend_prob += math.log(ncomment_words_prob[i])\n",
    "        else:\n",
    "            not_recommend_prob -= 10000\n",
    "    \n",
    "    title_words = test_data['title'][j].split(' ')\n",
    "    for i in title_words:\n",
    "        if i in title_words_prob:\n",
    "            recommend_prob += math.log(title_words_prob[i])\n",
    "        else:\n",
    "            recommend_prob -= 10000\n",
    "        if i in ntitle_words_prob:\n",
    "            not_recommend_prob += math.log(ntitle_words_prob[i])\n",
    "        else:\n",
    "            not_recommend_prob -= 10000\n",
    "        \n",
    "    \n",
    "    recommend_prob += math.log(p_recommendation)\n",
    "    not_recommend_prob += math.log(1-p_recommendation)\n",
    "\n",
    "    if recommend_prob > not_recommend_prob:\n",
    "        recom_pred += 1\n",
    "    if test_data['recommend'][j] == 'recommended' and recommend_prob > not_recommend_prob:\n",
    "        correct += 1\n",
    "        recom_correct += 1\n",
    "    if test_data['recommend'][j] == 'not_recommended' and recommend_prob < not_recommend_prob:\n",
    "        correct += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have four types of evaluation.\n",
    "### Accuracy\n",
    "Accuracy gives us the totall correct predictions devided by all comments.\n",
    "\n",
    "### Precision \n",
    "To calculating the precision, first we count the number of comments that predicted 'recommended' correctly and devide it to all the 'recommended' predicted comments. Obviously this is a evaluation type which give us the totall  efficiency becuase its only depend on one group of answers.\n",
    "\n",
    "### Recall\n",
    "Recall, correct 'recommended' detects devided by all actual 'recommended's. It's not valid on its own to. Excatly like the previuos one.\n",
    "\n",
    "### F1\n",
    "F1 is another measure of accuracy test. F1 score is the harmonic mean of the precision and recall.\n",
    "\n",
    "\n",
    "\n",
    "Both Precision and recall are not enaugh for testing the program. For instance if there is a sample that contains a small number of 'recommended' prediction, these two values may become near to 1 but it doesnt mean that the accuracy of totall program is good. These two can express the accuracy of the program when its prediction is a special value.('recomended')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.89\n",
      "Precision = 0.8513513513513513\n",
      "Recall = 0.945\n",
      "F1 = 0.8957345971563981\n"
     ]
    }
   ],
   "source": [
    "accuracy = correct / len(test_data)\n",
    "precision = recom_correct / recom_pred\n",
    "recall = recom_correct /  len(test_data[test_data['recommend'] == 'recommended'])\n",
    "f1 = 2 * (precision*recall) / (precision+recall)\n",
    "print(\"Accuracy =\", accuracy)\n",
    "print(\"Precision =\", precision)\n",
    "print(\"Recall =\", recall)\n",
    "print(\"F1 =\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More of Preprocesing\n",
    "There are many tasks before the main process that can help us improve are accuracy. Hazm library provides us some of these. One of them are Normalizer function which normalize each string. By using this function our accuracy drops down to 0.88. So we decided not to use it. The other function is Stemmer which remove the prepositions and so on. THIS IS AWFUL. It reduce the accuracy down to 0.80 !! we do not use this either. The other one is lemmatizer which is little bit better than other functions. It returns the root of each verb instead of the exact verb.This function make a little difference on the accuracy which is about 0.1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additive Smoothing\n",
    "In some cases we have new words in our test file that we havent seen in train comments. When this happens the probability of new word become 0 and the whole probability of the comment goes to 0. To solve the problem we add some a constant number(=1) to each word probability to numerator and total number of positive/negative words to the denominator.\n",
    "\n",
    "The most considerable change is when we add additive smoothing which improve our aaccuracy by 4%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final code\n",
    "Final program which provides us best accuracy is now here.\n",
    "Lets take a look at the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.91\n",
      "Precision = 0.9\n",
      "Recall = 0.9225\n",
      "F1 = 0.9111111111111112\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals\n",
    "from hazm import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "train_data = pd.read_csv(\"comment_train.csv\")\n",
    "p_recommendation = len(train_data[train_data['recommend'] == 'recommended']) / len(train_data['recommend'])\n",
    "t_recommendation = len(train_data[train_data['recommend'] == 'recommended'])\n",
    "n_recommendation = len(train_data[train_data['recommend'] == 'not_recommended'])\n",
    "###  Preprocessing Comments\n",
    "#  Recommended Comments\n",
    "normalizer = Normalizer()\n",
    "stemmer = Stemmer()\n",
    "lemmatizer = Lemmatizer()\n",
    "positive_comment_words = []\n",
    "for i in train_data[ train_data['recommend'] == 'recommended' ]['comment']:\n",
    "    #a = normalizer.normalize(i)\n",
    "    a = word_tokenize(i)\n",
    "    for j in a:\n",
    "        #positive_comment_words.append( stemmer.stem(j) )\n",
    "        positive_comment_words.append( lemmatizer.lemmatize(j) )\n",
    "\n",
    "total = len(positive_comment_words)\n",
    "comment_words_prob = {}\n",
    "for i in positive_comment_words:\n",
    "    if i in comment_words_prob:\n",
    "        comment_words_prob[i] += 1\n",
    "    else:\n",
    "        comment_words_prob[i] = 1\n",
    "\n",
    "for i in comment_words_prob:\n",
    "    comment_words_prob[i] = (comment_words_prob[i]+1) / (t_recommendation+total)\n",
    "\n",
    "# Not Recommended Comemnets\n",
    "negative_comment_words = []\n",
    "for i in train_data[ train_data['recommend'] == 'not_recommended' ]['comment']:\n",
    "    #a = normalizer.normalize(i)\n",
    "    a = word_tokenize(i)\n",
    "    for j in a:\n",
    "        #negative_comment_words.append(stemmer.stem( j ) )\n",
    "        negative_comment_words.append( lemmatizer.lemmatize(j) )\n",
    "\n",
    "total = len(negative_comment_words)\n",
    "ncomment_words_prob = {}\n",
    "for i in negative_comment_words:\n",
    "    if i in ncomment_words_prob:\n",
    "        ncomment_words_prob[i] += 1\n",
    "    else:\n",
    "        ncomment_words_prob[i] = 1\n",
    "\n",
    "for i in ncomment_words_prob:\n",
    "    ncomment_words_prob[i] = (ncomment_words_prob[i]+1) / (n_recommendation+total) \n",
    "#  Recommended Titles\n",
    "positive_title_words = []\n",
    "for i in train_data[ train_data['recommend'] == 'recommended' ]['title']:\n",
    "    #a = normalizer.normalize(i)\n",
    "    a = word_tokenize(i)\n",
    "    for j in a:\n",
    "        #positive_title_words.append( stemmer.stem(j))\n",
    "        positive_title_words.append( lemmatizer.lemmatize(j) )\n",
    "\n",
    "total = len(positive_title_words)\n",
    "title_words_prob = {}\n",
    "for i in positive_title_words:\n",
    "    if i in title_words_prob:\n",
    "        title_words_prob[i] += 1\n",
    "    else:\n",
    "        title_words_prob[i] = 1\n",
    "\n",
    "for i in title_words_prob:\n",
    "    title_words_prob[i] = (title_words_prob[i]+1) / (t_recommendation +total)\n",
    "\n",
    "#  Not Recommended Titles\n",
    "negative_title_words = []\n",
    "for i in train_data[ train_data['recommend'] == 'recommended' ]['title']:\n",
    "    #a = normalizer.normalize(i)\n",
    "    a = word_tokenize(i)\n",
    "    for j in a:\n",
    "        #negative_title_words.append( stemmer.stem(j) )\n",
    "        negative_title_words.append( lemmatizer.lemmatize(j) )\n",
    "\n",
    "total = len(negative_title_words)\n",
    "ntitle_words_prob = {}\n",
    "for i in negative_title_words:\n",
    "    if i in ntitle_words_prob:\n",
    "        ntitle_words_prob[i] += 1\n",
    "    else:\n",
    "        ntitle_words_prob[i] = 1\n",
    "\n",
    "for i in ntitle_words_prob:\n",
    "    ntitle_words_prob[i] = (ntitle_words_prob[i]+1) / (n_recommendation+total)\n",
    "\n",
    "\n",
    "### Prediction Phase\n",
    "\n",
    "test_data = pd.read_csv(\"comment_test.csv\")\n",
    "correct = 0\n",
    "recom_correct = 0\n",
    "recom_pred = 0\n",
    "tt = []\n",
    "cc = []\n",
    "for j in range( 1, len( test_data ) ):\n",
    "    ##comment_words = normalizer.normalize( test_data['comment'][j]) .split(' ')\n",
    "    comment_words = word_tokenize(test_data['comment'][j])\n",
    "    recommend_prob = 0\n",
    "    not_recommend_prob = 0\n",
    "    for i in comment_words:\n",
    "        #st = stemmer.stem(i) \n",
    "        st = lemmatizer.lemmatize(i)\n",
    "        if st in comment_words_prob:\n",
    "            recommend_prob += math.log(comment_words_prob[st])\n",
    "        else:\n",
    "            recommend_prob -= 10000\n",
    "        if st in ncomment_words_prob:\n",
    "            not_recommend_prob += math.log(ncomment_words_prob[st])\n",
    "        else:\n",
    "            not_recommend_prob -= 10000\n",
    "    \n",
    "    #title_words = normalizer.normalize( test_data['title'][j] ).split(' ')\n",
    "    title_words = test_data['title'][j]\n",
    "    for i in title_words:\n",
    "        #st = stemmer.stem(i) \n",
    "        st = lemmatizer.lemmatize(i)\n",
    "        if st in title_words_prob:\n",
    "            recommend_prob += math.log(title_words_prob[st])\n",
    "        else:\n",
    "            recommend_prob -= 10000\n",
    "        if st in ntitle_words_prob:\n",
    "            not_recommend_prob += math.log(ntitle_words_prob[st])\n",
    "        else:\n",
    "            not_recommend_prob -= 10000\n",
    "    \n",
    "    recommend_prob += math.log(p_recommendation)\n",
    "    not_recommend_prob += math.log(1-p_recommendation)\n",
    "\n",
    "    if recommend_prob > not_recommend_prob:\n",
    "        recom_pred += 1\n",
    "    if test_data['recommend'][j] == 'recommended' and recommend_prob > not_recommend_prob:\n",
    "        correct += 1\n",
    "        recom_correct += 1\n",
    "    if test_data['recommend'][j] == 'not_recommended' and recommend_prob < not_recommend_prob:\n",
    "        correct += 1\n",
    "    \n",
    "    if (test_data['recommend'][j] == 'not_recommended' and recommend_prob > not_recommend_prob ) or (test_data['recommend'][j] == 'recommended' and recommend_prob < not_recommend_prob):\n",
    "        tt.append(\"Title = \" + test_data['title'][j])\n",
    "        cc.append(\"Comment = \" + test_data['comment'][j])\n",
    "\n",
    "\n",
    "accuracy = correct / len(test_data)\n",
    "precision = recom_correct / recom_pred\n",
    "recall = recom_correct /  len(test_data[test_data['recommend'] == 'recommended'])\n",
    "f1 = 2 * (precision*recall) / (precision+recall)\n",
    "print(\"Accuracy =\", accuracy)\n",
    "print(\"Precision =\", precision)\n",
    "print(\"Recall =\", recall)\n",
    "print(\"F1 =\", f1)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Table\n",
    "This part goes to describing the accuracy of additive smoothing and preprocessing functions. It is clear that additive smoothing is far more effective than preprocessing functions.\n",
    "\n",
    "|           | a      | b      | c    | d     |\n",
    "| --------- | ------ | ------ | ---- | ----- |\n",
    "| Accuracy  | 0.9112 | 0.91   | 0.87 | 0.89  |\n",
    "| Precision | 0.898  | 0.9    | 0.82 | 0.851 |\n",
    "| Recall    | 0.927  | 0.9225 | 0.95 | 0.945 |\n",
    "| F1        | 0.912  | 0.9111 | 0.88 | 0.895 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here are some comments that our program can not predict it correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title = نقد پس از خرید\n",
      "Comment = سلام ، راحت شدم از کابل شارژ ، توصیه میشود به شدت . ارزان گوشی خود را به شارژ وایرلس مجهز کنید .\n",
      "\n",
      "Title = خیالم راحت شد\n",
      "Comment = فندک قبلیم مدام فیوز میسوزوند و یک بار شارژر موبایل هم سوزوند ولی با این هیچ مشکلی بوجود نیومده تا الان. کیفیتش خیلی خوبه و لامپ هم داره\n",
      "\n",
      "Title = محصول ضعیفی هست\n",
      "Comment = بدون پخش بو و ماندگاری زیاد امیدوارم کیفیتش رو ارتقا بدن\n",
      "\n",
      "Title = نخرید خوب نیس اصلا\n",
      "Comment = با سلام، با چیزی که توی عکس هس فرق داره، اصلا روی گوشی درست واینمیسته، ی لبه گوشی خالی میمونه نمیرسه ب ال سی دی، من ک مرجوع کردم\n",
      "\n",
      "Title = بررسی فیلتر سرکان\n",
      "Comment = من خودم جزو افرادی بودم که نزدیک سیزده ساله از انواع فیلتر سرکان اعم از روغن، هوا و اتاق استفاده میکردم ولی به تازگی متوجه و اطلاع یافتم که فیلتر گاج باکیفیت تر از فیلتر سرکان می‌باشد و هم چنین قیمت بمراتب مناسب تربت نسبت به سرکان دارد و طرف فروشنده که داشت روغن فیلترش را به من می‌فروخت. واقعا بهم اثبات کرد که گاج باکیفیت تر از سرکان می‌باشد.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,6):\n",
    "    print(tt[i])\n",
    "    print(cc[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ambiguous words may cuase the wrong predictions. Although we have additive smoothing but new words may be a point for wrong predictions. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
